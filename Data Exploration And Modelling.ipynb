{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Data Scientist Task\n",
    "\n",
    "Given the following data:\n",
    "Messages (Attached)\n",
    "Relationships (Attached)\n",
    "\n",
    "Create a quick model to predict whether or not a message is written for a Birthday and whom it is\n",
    "being sent to, if it is, class it as a Birthday and pull the relationship data for the user id and display the\n",
    "age of the recipient from the relationship data.\n",
    "Output should be something along the lines of:\n",
    "Birthday - Mum - 65\n",
    "Not birthday - Uncle - Nan\n",
    "Birthday - Mum - Nan\n",
    "ETC\n",
    "**To be aware of:\n",
    "1. Please provide the actual code/notebook you used to create this model - preferably through\n",
    "GitHub or similar.\n",
    "2. Should be able to run through an example in real time during the interview (just one\n",
    "message added in with a user id known to have a relationship)\n",
    "3. Think about latency/speed where possible\n",
    "4. Try not to focus too much on accuracy at this point in order to save time in completing the\n",
    "project, however, we will be looking at coding principles to assess\n",
    "5. Please come prepared to answer technical questions on the model, we will be looking for\n",
    "crystal clear communication.\n",
    "6. Not all data will be present for relationship birthdays and so fill with Nan where not possible\n",
    "to get age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "1. Data Exploration\n",
    "2. Data Cleaning\n",
    "3. Merging of different datasets\n",
    "4. Building  models,  for birthday.\n",
    "\n",
    "At the end our input should be of the following json format\n",
    "{\"msg\":string,\"user_id\":string)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "1. Basic stats of data for message and relationships\n",
    "2. Dropping Nulls\n",
    "3. Identifying what features can be developed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vikash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vikash/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, f1_score\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.externals import joblib\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import nlpaug.augmenter.word as naw\n",
    "import random\n",
    "import stop_words\n",
    "from nltk.stem import PorterStemmer \n",
    "stops =stop_words.get_stop_words(language='en')\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "ps = PorterStemmer() \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions\n",
    "\n",
    "Functions classes used in the modelling process have been listed down here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_text(x):\n",
    "    text_list =[]\n",
    "    x_eval = eval(x)\n",
    "    for i in range(len(x_eval)):\n",
    "        text_list.append(x_eval[i]['text'])\n",
    "    try:\n",
    "        return ' '.join(text_list)\n",
    "    except TypeError:\n",
    "        return ' '\n",
    "    \n",
    "    \n",
    "def f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    mean squared error for y_true and y_predicted\n",
    "    y_true: True values of array\n",
    "    y_pred: Predicted values from a model\n",
    "    \"\"\"\n",
    "#     pred1 = [p[0] for p in y_pred]\n",
    "#     pred2 = [p[1] for p in y_pred]\n",
    "    f1_rel = f1_score(y_pred,y_true,average='binary')\n",
    "#     f1_birth = f1_score(pred2,y_true['birthday_trgt'],average='binary')\n",
    "    return f1_rel\n",
    "f1_scorer = make_scorer(f1)\n",
    "\n",
    "#feature cleaner\n",
    "class FeatureCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, clean=True):\n",
    "        self.clean = clean\n",
    "        \n",
    "    def clean_and_normalize_text_data(self,sentence):\n",
    "        sentence = re.sub(r'[^a-zA-Z\\s]', ' ', sentence, re.I|re.A)\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.strip()\n",
    "        tokens = wpt.tokenize(sentence)\n",
    "        stemmed_words = [ps.stem(w) for w in tokens]\n",
    "        #remove stopwords\n",
    "        filtered_tokens = [token for token in stemmed_words if token not in stops]\n",
    "        filtered_len =[token for token in filtered_tokens if len(token)>2]\n",
    "        filtered_len = np.unique(filtered_len)\n",
    "        sentence = ' '.join(filtered_len)\n",
    "        return sentence\n",
    "        \n",
    "\n",
    "    def transform(self, X,y=None):\n",
    "        c =pd.Series([self.clean_and_normalize_text_data(x) for x in X])\n",
    "        return c\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    " #statistical features generated   \n",
    "class FeatureMultiplierCount(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, word_count=True,char_count=True,\n",
    "                word_density=True,total_length=True,\n",
    "                capitals=True,caps_vs_length=True,num_exclamation_marks=True,num_question_marks=True,\n",
    "                num_punctuation=True,num_symbols=True,num_unique_words=True,words_vs_unique=True,\n",
    "                word_unique_percent=True):\n",
    "        self.word_count = word_count\n",
    "        self.total_length = total_length\n",
    "        self.char_count =char_count\n",
    "        self.word_density = word_density\n",
    "        self.capitals = capitals\n",
    "        self.caps_vs_length = caps_vs_length\n",
    "        self.num_exclamation_marks=num_exclamation_marks\n",
    "        self.num_question_marks=num_question_marks\n",
    "        self.num_punctuation=num_punctuation\n",
    "        self.num_symbols=num_symbols\n",
    "        self.num_unique_words = num_unique_words\n",
    "        self.words_vs_unique = words_vs_unique\n",
    "        self.word_unique_percent = word_unique_percent\n",
    "\n",
    "    def transform(self, X,y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        X['word_count'] = X['all_text_new'].apply(lambda x : len(x.split()))\n",
    "        X['char_count'] = X['all_text_new'].apply(lambda x : len(x.replace(\" \",\"\")))\n",
    "        X['word_density'] = X['word_count'] / (X['char_count'] + 1)\n",
    "\n",
    "        X['total_length'] = X['all_text_new'].apply(len)\n",
    "        X['capitals'] = X['all_text_new'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    " \n",
    "        X['num_exclamation_marks'] =X['all_text_new'].apply(lambda x: x.count('!'))\n",
    "        X['num_question_marks'] = X['all_text_new'].apply(lambda x: x.count('?'))\n",
    "        X['num_punctuation'] = X['all_text_new'].apply(lambda x: sum(x.count(w) for w in '.,;:'))\n",
    "        X['num_symbols'] = X['all_text_new'].apply(lambda x: sum(x.count(w) for w in '*&$%'))\n",
    "        X['num_unique_words'] = X['all_text_new'].apply(lambda x: len(set(w for w in x.split())))\n",
    "       \n",
    "        \n",
    "        return X[['word_count','char_count','word_density','total_length',\n",
    "                 'capitals','num_exclamation_marks','num_question_marks',\n",
    "                 'num_punctuation','num_symbols','num_unique_words']]\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "\n",
    "#modelling pipeline \n",
    "def models():\n",
    "    \"\"\"\n",
    "    \n",
    "    returns three pipelines Random Forest, Adaboost, Gradientboost,SVC\n",
    "    The scoring function is based on f1_score\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Random Forest Pipeline\n",
    "    rf_pipeline = Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([('clean',FeatureCleaner()),\n",
    "             ('tfidf', TfidfVectorizer(max_features=10000,ngram_range=(1,3))),\n",
    "        ])),\n",
    "        ('numerical_features',Pipeline([('numerical_feats',FeatureMultiplierCount()),\n",
    "                                       ('scaler',StandardScaler()),\n",
    "                                       ])),\n",
    "\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "\n",
    "])\n",
    "    # Adaboost Pipeline\n",
    "    AdaBoost_pipeline = Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([('clean',FeatureCleaner()),\n",
    "             ('tfidf', TfidfVectorizer(max_features=10000,ngram_range=(1,3))),\n",
    "        ])),\n",
    "        ('numerical_features',Pipeline([('numerical_feats',FeatureMultiplierCount()),\n",
    "                                       ('scaler',StandardScaler()),\n",
    "                                       ])),\n",
    "\n",
    "    ])),\n",
    "    ('clf', AdaBoostClassifier()),\n",
    "\n",
    "])\n",
    "    # Gradient Boost Pipeline\n",
    "    GRD_pipeline = Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([('clean',FeatureCleaner()),\n",
    "             ('tfidf', TfidfVectorizer(max_features=10000,ngram_range=(1,3))),\n",
    "        ])),\n",
    "        ('numerical_features',Pipeline([('numerical_feats',FeatureMultiplierCount()),\n",
    "                                       ('scaler',StandardScaler()),\n",
    "                                       ])),\n",
    "\n",
    "    ])),\n",
    "    ('clf', GradientBoostingClassifier()),\n",
    "\n",
    "])\n",
    "    \n",
    "    svm_pipeline =Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([('clean',FeatureCleaner()),\n",
    "             ('tfidf', TfidfVectorizer(max_features=10000,ngram_range=(1,3))),\n",
    "        ])),\n",
    "        ('numerical_features',Pipeline([('numerical_feats',FeatureMultiplierCount()),\n",
    "                                       ('scaler',StandardScaler()),\n",
    "                                       ])),\n",
    "\n",
    "    ])),\n",
    "    ('clf', LinearSVC()),\n",
    "\n",
    "])\n",
    "    #grid search params for randomforest, adaboost, gradientboost\n",
    "    grid_params_rf = [{'clf__n_estimators': [10, 50, 100], 'clf__max_depth': [2, 3, 5]}]\n",
    "    grid_params_adaboost = [{'clf__n_estimators': [10, 50, 100,500], 'clf__learning_rate': [0.5, 0.8, 1.0]}]\n",
    "    grid_params_grd = [{'clf__n_estimators': [10, 50, 100,500], 'clf__learning_rate': [0.5, 0.8, 1.0],\n",
    "                            'clf__max_depth': [2, 3, 5]}]\n",
    "    \n",
    "    grid_params_svc = [{'clf__C': [1.0,3.0,5.0,10.0],'clf__max_iter':[1000]}]\n",
    "\n",
    "    #gridsearchcv pipeline for randomforest\n",
    "    gs_rf = GridSearchCV(estimator=rf_pipeline,\n",
    "                             param_grid=grid_params_rf,\n",
    "                             scoring=f1_scorer,\n",
    "                             cv=5)\n",
    "    #gridsearchcv pipeline for adaboost\n",
    "    gs_adaboost = GridSearchCV(estimator=AdaBoost_pipeline,\n",
    "                                   param_grid=grid_params_adaboost,\n",
    "                                   scoring=f1_scorer,\n",
    "                                   cv=5)\n",
    "\n",
    "    #gridsearchcv pipeline for gradientboost\n",
    "    gs_grd = GridSearchCV(estimator=GRD_pipeline,\n",
    "                              param_grid=grid_params_grd,\n",
    "                              scoring=f1_scorer,\n",
    "                              cv=5)\n",
    "    gs_svc = GridSearchCV(estimator=svm_pipeline,\n",
    "                              param_grid=grid_params_svc,\n",
    "                              scoring=f1_scorer,\n",
    "                              cv=5)\n",
    "\n",
    "    grids = [gs_svc,gs_rf, gs_adaboost, gs_grd]\n",
    "    # grids = [gs_rf]\n",
    "    return grids\n",
    "\n",
    "\n",
    "def clean_and_normalize_text_data(sent):\n",
    "\n",
    "\n",
    "    \n",
    "    sent_cleaned = sent.replace('\\n','')\n",
    "    return sent_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_uuid</th>\n",
       "      <th>message</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>[{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>[{\"text\":\"It's going well....\",\"fontIntegratio...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f841ccb1-3599-a847-d99b-150a9cb19858</td>\n",
       "      <td>[{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>[{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4407500-2e8e-8059-04b5-12adb394ac0d</td>\n",
       "      <td>[{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              user_uuid  \\\n",
       "0  34c87972-e471-4e9b-aefc-d5778a9ae505   \n",
       "1  117fd8a2-5da2-8b13-6df0-d6766d849093   \n",
       "2  f841ccb1-3599-a847-d99b-150a9cb19858   \n",
       "3  117fd8a2-5da2-8b13-6df0-d6766d849093   \n",
       "4  d4407500-2e8e-8059-04b5-12adb394ac0d   \n",
       "\n",
       "                                             message Unnamed: 2  \n",
       "0  [{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...        NaN  \n",
       "1  [{\"text\":\"It's going well....\",\"fontIntegratio...        NaN  \n",
       "2  [{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...        NaN  \n",
       "3  [{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...        NaN  \n",
       "4  [{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...        NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_dt = pd.read_csv('Message.csv')\n",
    "rel = pd.read_csv('relationships.csv')\n",
    "msg_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      "user_uuid     10000 non-null object\n",
      "message       10000 non-null object\n",
      "Unnamed: 2    346 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#basic info about message data\n",
    "msg_dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQUE USER IDS IN MESSAGE DATA 8600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cb66eaa6-cb85-f180-f2df-b26b91072e76    83\n",
       "45903ea1-3d81-7f7d-25cd-9acc5c4570f3    35\n",
       "65dc5608-0fde-241f-e963-9ed255703a83    21\n",
       "4b2b3920-0498-100e-6022-5d29ac36bdb2    17\n",
       "3f037c21-dbe1-cb56-c080-1e28d71f0543    16\n",
       "                                        ..\n",
       "ba0e326c-e465-419e-b6d1-056683ea2347     1\n",
       "ebba6f18-3ff4-4748-8f09-4ae7973037d6     1\n",
       "df841ee5-72ef-8c41-8623-ef6dfc218dcd     1\n",
       "905bd8ff-56dc-4cb0-b766-61d300544afa     1\n",
       "b2eb420b-4794-4669-80f3-378d00b9682c     1\n",
       "Name: user_uuid, Length: 8600, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"UNIQUE USER IDS IN MESSAGE DATA {}\".format(msg_dt['user_uuid'].nunique()))\n",
    "msg_dt['user_uuid'].value_counts()\n",
    "#the value counts on user ids shows the top users with their number of messages sent.\n",
    "#lets explore on few users using the relationship data where they most likely use touchnote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_uuid</th>\n",
       "      <th>message</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>message_new</th>\n",
       "      <th>all_text</th>\n",
       "      <th>all_text_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>[{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"Dear\",\"fontIntegration\":0,\"font\":\"Bl...</td>\n",
       "      <td>Dear Granddad Get well soon!\\n\\nWe love you th...</td>\n",
       "      <td>Dear Granddad Get well soon!We love you the wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>[{\"text\":\"It's going well....\",\"fontIntegratio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"It's going well....\",\"fontIntegratio...</td>\n",
       "      <td>It's going well....</td>\n",
       "      <td>It's going well....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f841ccb1-3599-a847-d99b-150a9cb19858</td>\n",
       "      <td>[{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...</td>\n",
       "      <td>Liebe Rita &amp; Claudia,\\n\\nliebe und sonnige Grü...</td>\n",
       "      <td>Liebe Rita &amp; Claudia,liebe und sonnige Grüße a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              user_uuid  \\\n",
       "0  34c87972-e471-4e9b-aefc-d5778a9ae505   \n",
       "1  117fd8a2-5da2-8b13-6df0-d6766d849093   \n",
       "2  f841ccb1-3599-a847-d99b-150a9cb19858   \n",
       "\n",
       "                                             message Unnamed: 2  \\\n",
       "0  [{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...        NaN   \n",
       "1  [{\"text\":\"It's going well....\",\"fontIntegratio...        NaN   \n",
       "2  [{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...        NaN   \n",
       "\n",
       "                                         message_new  \\\n",
       "0  [{\"text\":\"Dear\",\"fontIntegration\":0,\"font\":\"Bl...   \n",
       "1  [{\"text\":\"It's going well....\",\"fontIntegratio...   \n",
       "2  [{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...   \n",
       "\n",
       "                                            all_text  \\\n",
       "0  Dear Granddad Get well soon!\\n\\nWe love you th...   \n",
       "1                                It's going well....   \n",
       "2  Liebe Rita & Claudia,\\n\\nliebe und sonnige Grü...   \n",
       "\n",
       "                                        all_text_new  \n",
       "0  Dear Granddad Get well soon!We love you the wo...  \n",
       "1                                It's going well....  \n",
       "2  Liebe Rita & Claudia,liebe und sonnige Grüße a...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we format the message data to extract all the text to be used further in modelling, removing the notorious '\\n' also\n",
    "msg_dt['message_new'] = msg_dt['message'].apply(lambda x: x.replace('null',str(0)))\n",
    "msg_dt['all_text_new'] = msg_dt['message_new'].apply(lambda x:extract_all_text(x))\n",
    "msg_dt['all_text_new'] = msg_dt['all_text_new'].apply(lambda x:clean_and_normalize_text_data(x))\n",
    "msg_dt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_uuid</th>\n",
       "      <th>birthday</th>\n",
       "      <th>relationship</th>\n",
       "      <th>relationship_new</th>\n",
       "      <th>birthday_trgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>17/06/1938 00:00</td>\n",
       "      <td>PARENTS</td>\n",
       "      <td>PARENTS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MOM</td>\n",
       "      <td>MOM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f841ccb1-3599-a847-d99b-150a9cb19858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SISTER_IN_LAW</td>\n",
       "      <td>SISTER_IN_LAW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MOM</td>\n",
       "      <td>MOM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4407500-2e8e-8059-04b5-12adb394ac0d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DAD</td>\n",
       "      <td>DAD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              user_uuid          birthday   relationship  \\\n",
       "0  34c87972-e471-4e9b-aefc-d5778a9ae505  17/06/1938 00:00        PARENTS   \n",
       "1  117fd8a2-5da2-8b13-6df0-d6766d849093               NaN            MOM   \n",
       "2  f841ccb1-3599-a847-d99b-150a9cb19858               NaN  SISTER_IN_LAW   \n",
       "3  117fd8a2-5da2-8b13-6df0-d6766d849093               NaN            MOM   \n",
       "4  d4407500-2e8e-8059-04b5-12adb394ac0d               NaN            DAD   \n",
       "\n",
       "  relationship_new  birthday_trgt  \n",
       "0          PARENTS              1  \n",
       "1              MOM              0  \n",
       "2    SISTER_IN_LAW              0  \n",
       "3              MOM              0  \n",
       "4              DAD              0  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#relationship data\n",
    "rel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_uuid                             relationship      \n",
       "45903ea1-3d81-7f7d-25cd-9acc5c4570f3  BEST_FRIEND_FEMALE     1\n",
       "                                      Blipfoto Friend        6\n",
       "                                      COLLEAGUE_FEMALE       1\n",
       "                                      COLLEAGUE_MALE         1\n",
       "                                      COUSIN                 3\n",
       "                                      Church Friend          3\n",
       "                                      GRANDSON               8\n",
       "                                      MS Friend              2\n",
       "                                      Neighbour              1\n",
       "                                      SISTER                 1\n",
       "                                      Sister-in-law          1\n",
       "                                      Visiting Sister        7\n",
       "65dc5608-0fde-241f-e963-9ed255703a83  BEST_FRIEND_FEMALE    19\n",
       "                                      FRIEND_FEMALE          2\n",
       "cb66eaa6-cb85-f180-f2df-b26b91072e76  BEST_FRIEND_FEMALE     2\n",
       "                                      BEST_FRIEND_MALE       3\n",
       "                                      BOYFRIEND              3\n",
       "                                      COLLEAGUE_FEMALE       2\n",
       "                                      COLLEAGUE_MALE         6\n",
       "                                      DAD                   19\n",
       "                                      FRIENDS                7\n",
       "                                      FRIEND_FEMALE          2\n",
       "                                      FRIEND_MALE            2\n",
       "                                      GIRLFRIEND             1\n",
       "                                      GRANDMA                9\n",
       "                                      MOM                   16\n",
       "                                      NURSE                  8\n",
       "                                      PARENTS                2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting couple of top users \n",
    "users  =['cb66eaa6-cb85-f180-f2df-b26b91072e76','45903ea1-3d81-7f7d-25cd-9acc5c4570f3','65dc5608-0fde-241f-e963-9ed255703a83']\n",
    "user_relationship = rel[rel['user_uuid'].isin(users)]\n",
    "user_relationship.groupby(['user_uuid','relationship']).size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights About The Users, Can it be automated ?\n",
    "\n",
    "1. 45903ea1-3d81-7f7d-25cd-9acc5c4570f3, the relationship reveals a lot about the age intervals, the user has messaged 8times to 'GRANDSON' the age can be put in the interval of (>45). Though anomalies can always happen. The social structure is also kind of predictive, from the messaging list we can observe anomaly relationship like \"Church Friend\", in this world of very private relationships, having a \"Church Friend\" reveals a very wide and friendly network.\n",
    "\n",
    "2. 65dc5608-0fde-241f-e963-9ed255703a83, in contrast to the above user this particular user has a very secluded list, not much to get from here.\n",
    "\n",
    "3. cb66eaa6-cb85-f180-f2df-b26b91072e76, this user is probably young \"GRANDMA\" and \"GIRLFRIEND\" indicates that and is not married. He was also probably in hospital recently(\"NURSE\").\n",
    "\n",
    "These insights can be automated in multiple ways:\n",
    "1. Connecting nodes(relationships) to messages for each user with weights as number of times the message has been sent, in turn entities from these messages are connected to all users.\n",
    "\n",
    "This type of graphical relationship will allow us to harness these insights in automated way.\n",
    "\n",
    "Here I would also like to comment that having message_id to each message and connecting the message_id to relationship\n",
    "would have greater influence. Since a user might message multiple times, and which message corresponds to which relationship will have richer represenatation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9892, 6)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtering messages with no text\n",
    "msg_dt = msg_dt[msg_dt['all_text_new']!=' ']\n",
    "msg_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_uuid</th>\n",
       "      <th>message</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>message_new</th>\n",
       "      <th>all_text</th>\n",
       "      <th>all_text_new</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>[{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"Dear\",\"fontIntegration\":0,\"font\":\"Bl...</td>\n",
       "      <td>Dear Granddad Get well soon!\\n\\nWe love you th...</td>\n",
       "      <td>Dear Granddad Get well soon!We love you the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>[{\"text\":\"It's going well....\",\"fontIntegratio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"It's going well....\",\"fontIntegratio...</td>\n",
       "      <td>It's going well....</td>\n",
       "      <td>It's going well....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f841ccb1-3599-a847-d99b-150a9cb19858</td>\n",
       "      <td>[{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...</td>\n",
       "      <td>Liebe Rita &amp; Claudia,\\n\\nliebe und sonnige Grü...</td>\n",
       "      <td>Liebe Rita &amp; Claudia,liebe und sonnige Grüße a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>[{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...</td>\n",
       "      <td>\\n\\n\\n\\nYou can't see the chilli and parsnip c...</td>\n",
       "      <td>You can't see the chilli and parsnip chutney a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4407500-2e8e-8059-04b5-12adb394ac0d</td>\n",
       "      <td>[{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...</td>\n",
       "      <td>¡Hola papá!\\n\\nMira qué vistas desde lo alto d...</td>\n",
       "      <td>¡Hola papá!Mira qué vistas desde lo alto del P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              user_uuid  \\\n",
       "0  34c87972-e471-4e9b-aefc-d5778a9ae505   \n",
       "1  117fd8a2-5da2-8b13-6df0-d6766d849093   \n",
       "2  f841ccb1-3599-a847-d99b-150a9cb19858   \n",
       "3  117fd8a2-5da2-8b13-6df0-d6766d849093   \n",
       "4  d4407500-2e8e-8059-04b5-12adb394ac0d   \n",
       "\n",
       "                                             message Unnamed: 2  \\\n",
       "0  [{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...        NaN   \n",
       "1  [{\"text\":\"It's going well....\",\"fontIntegratio...        NaN   \n",
       "2  [{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...        NaN   \n",
       "3  [{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...        NaN   \n",
       "4  [{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...        NaN   \n",
       "\n",
       "                                         message_new  \\\n",
       "0  [{\"text\":\"Dear\",\"fontIntegration\":0,\"font\":\"Bl...   \n",
       "1  [{\"text\":\"It's going well....\",\"fontIntegratio...   \n",
       "2  [{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...   \n",
       "3  [{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...   \n",
       "4  [{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...   \n",
       "\n",
       "                                            all_text  \\\n",
       "0  Dear Granddad Get well soon!\\n\\nWe love you th...   \n",
       "1                                It's going well....   \n",
       "2  Liebe Rita & Claudia,\\n\\nliebe und sonnige Grü...   \n",
       "3  \\n\\n\\n\\nYou can't see the chilli and parsnip c...   \n",
       "4  ¡Hola papá!\\n\\nMira qué vistas desde lo alto d...   \n",
       "\n",
       "                                        all_text_new  target  \n",
       "0  Dear Granddad Get well soon!We love you the wo...       0  \n",
       "1                                It's going well....       0  \n",
       "2  Liebe Rita & Claudia,liebe und sonnige Grüße a...       0  \n",
       "3  You can't see the chilli and parsnip chutney a...       0  \n",
       "4  ¡Hola papá!Mira qué vistas desde lo alto del P...       0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a birthday target variable\n",
    "def brth_(x):\n",
    "    split_ = x.split(' ')\n",
    "    if len(set(split_).intersection(set(['birthday','Birthday'])))>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "msg_dt['target'] = msg_dt['all_text_new'].apply(lambda x:brth_(x))\n",
    "msg_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.91943\n",
      "1    0.08057\n",
      "Name: target, dtype: float64\n",
      "(9892, 7)\n"
     ]
    }
   ],
   "source": [
    "#target variables percentage\n",
    "print (msg_dt['target'].value_counts(normalize=True))\n",
    "print (msg_dt.shape)\n",
    "#they seem okay to proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_dt = msg_dt[['user_uuid','target','all_text_new']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Augmentation For Oversampling\n",
    "\n",
    "In traditional modelling methods we used various oversampling methods after featurization to increase the undersampled classes.\n",
    "But in modern methods the concept of text augmentation has come around to good usage. The concept of text augmentation is to generate similar meaning text but by replacing new words so that we can have more text data for undersampled classes. We can use the following approaches for text augmentation to generate good relevant text:\n",
    "\n",
    "1. Replace some words by their synonyms\n",
    "2. Replace some words by similar words(like love is replaced beloved)\n",
    "3. Translate the sentence to other language and then translate it back to english\n",
    "\n",
    "We will be using the second number point. We will pull every undersampled data to atleast 10%(around 200) of the highest class is they are'nt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we try to keep the classes which have lower data to atleast 30% i.e 2966 datapoints of the highest class, i.e each class must have 20% of highest class of data.\n",
    "#for that we augment text using synonyms like below\n",
    "aug  = naw.SynonymAug(aug_src='wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augmented_text(classes):\n",
    "    df = msg_dt[msg_dt['target']==classes]\n",
    "    print (df.shape)\n",
    "    all_ids = df['user_uuid'].to_list()\n",
    "    max_num =2966\n",
    "    to_augment = max_num-df.shape[0]\n",
    "    print (to_augment)\n",
    "    new_df = pd.DataFrame()\n",
    "    id_chosen =[]\n",
    "    i=0\n",
    "    while i<to_augment:\n",
    "        id_ = random.choice(list(set(all_ids)-set(id_chosen)))\n",
    "        sent = df[df['user_uuid']==id_]['all_text_new'].to_list()[0]\n",
    "        sent_generated = aug.augment(sent)\n",
    "        temp = pd.DataFrame(columns=['user_uuid','target','all_text_new'],index=[0])\n",
    "        temp['user_uuid'] =id_\n",
    "        temp['target'] = classes\n",
    "      \n",
    "        temp['all_text_new'] = sent_generated\n",
    "        new_df = new_df.append(temp)\n",
    "        i=i+1\n",
    "    return pd.concat([df,new_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(797, 3)\n",
      "2169\n"
     ]
    }
   ],
   "source": [
    "aug_class =[1]\n",
    "\n",
    "new_aug_dt =[]\n",
    "for cl in aug_class:\n",
    "    df = generate_augmented_text(cl)\n",
    "    new_aug_dt.append(df)\n",
    "train_dt = msg_dt[~msg_dt['target'].isin(aug_class)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12061, 3)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after oversampling with text augmentation\n",
    "all_dt = pd.concat([train_dt,new_aug_dt[0]])\n",
    "all_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.754083\n",
       "1    0.245917\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dt['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is the features on which we develop and y is the target variable\n",
    "y = all_dt.target\n",
    "X = all_dt.all_text_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test and train in 75% and 25% ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 9095, 1: 2966})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization And Modelling\n",
    "\n",
    "1. For featurization we are using the following features:\n",
    " 1. TFIDF(10000) features with ngram(1,3) \n",
    " 2. Counting features\n",
    " 3. Scaling the count features \n",
    " 4. Combining all the features\n",
    "\n",
    "2. Modelling\n",
    "   1. We use LinearSVC, Randomforest, Gradientboost, Adaboost to train the model\n",
    "   2. We run our gridsearchcv with cv=5 \n",
    "   3. We choose the best model on F1 score\n",
    "   \n",
    "All this is being done in a single pipeline(refer models function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimator: svc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__C': 1.0, 'clf__max_iter': 1000}\n",
      "Best training F1 score  score: 0.934\n",
      "Test set f1score for best params: 0.933 \n",
      "\n",
      "Estimator: rf\n",
      "Best params: {'clf__max_depth': 5, 'clf__n_estimators': 10}\n",
      "Best training F1 score  score: 0.030\n",
      "Test set f1score for best params: 0.000 \n",
      "\n",
      "Estimator: ada\n",
      "Best params: {'clf__learning_rate': 0.5, 'clf__n_estimators': 50}\n",
      "Best training F1 score  score: 0.927\n",
      "Test set f1score for best params: 0.912 \n",
      "\n",
      "Estimator: grd\n",
      "Best params: {'clf__learning_rate': 0.5, 'clf__max_depth': 3, 'clf__n_estimators': 10}\n",
      "Best training F1 score  score: 0.924\n",
      "Test set f1score for best params: 0.906 \n",
      "\\Classifier with best test set f1 score: svc\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "grid_dict = {0:'svc',1:'rf',2:'ada',3:'grd'}\n",
    "\n",
    "    # grid_dict = {0: 'rf'}\n",
    "\n",
    "grids =models()\n",
    "\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    print('Best training F1 score  score: %.3f' % gs.best_score_)\n",
    "    y_pred = gs.predict(X_test)\n",
    "    print('Test set f1score for best params: %.3f ' % f1(y_test, y_pred))\n",
    "    if f1(y_test, y_pred) >best_acc:\n",
    "        best_acc = f1(y_test, y_pred)\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\Classifier with best test set f1 score: %s' % grid_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "# since the best pipeline is with LinearSVC we will run and store it on the complete dataset\n",
    "svm_pipeline =Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([('clean',FeatureCleaner()),\n",
    "             ('tfidf', TfidfVectorizer(max_features=10000,ngram_range=(1,3))),\n",
    "        ])),\n",
    "        ('numerical_features',Pipeline([('numerical_feats',FeatureMultiplierCount()),\n",
    "                                       ('scaler',StandardScaler()),\n",
    "                                       ])),\n",
    "\n",
    "    ])),\n",
    "    ('clf', LinearSVC()),\n",
    "\n",
    "])\n",
    "grid_params_svc = [{'clf__C': [1.0,3.0,5.0,10.0],'clf__max_iter':[1000]}]\n",
    "#gridsearchcv pipeline for LinearSVC\n",
    "gs_svc = GridSearchCV(estimator=svm_pipeline,\n",
    "                              param_grid=grid_params_svc,\n",
    "                              scoring=f1_scorer,\n",
    "                              cv=5)\n",
    "\n",
    "model = gs_svc.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9259141836256134"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_gs_pipeline_brthday_svc.pkl']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_file = 'best_gs_pipeline_brthday_svc.pkl'\n",
    "\n",
    "\n",
    "joblib.dump(model.best_estimator_,  dump_file, compress=1)\n",
    "\n",
    "# with open(model.best_estimator_, 'wb') as file:\n",
    "#     pickle.dump(best_gs, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib_clf = joblib.load(dump_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HAPPY BIRTHDAY! I love you cousin Ella! ! Seminal fluid gambling in my ball quarry with me, we cause sooooo much fun! Anyway has a happy birthday I hope you get lots of cake! ! Love, Aria'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dt = pd.read_csv(\"relationships.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_uuid</th>\n",
       "      <th>birthday</th>\n",
       "      <th>relationship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>17/06/1938 00:00</td>\n",
       "      <td>PARENTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>18/02/1939 00:00</td>\n",
       "      <td>MOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>14/08/1965 00:00</td>\n",
       "      <td>SISTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>14/08/1995 00:00</td>\n",
       "      <td>FRIEND_MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>18/02/1939 00:00</td>\n",
       "      <td>MOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>18/02/1938 00:00</td>\n",
       "      <td>MOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>22/08/1963 00:00</td>\n",
       "      <td>SISTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>17/06/1938 00:00</td>\n",
       "      <td>PARENTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>09/08/1957 00:00</td>\n",
       "      <td>BEST_FRIEND_FEMALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 user_uuid          birthday  \\\n",
       "0     34c87972-e471-4e9b-aefc-d5778a9ae505  17/06/1938 00:00   \n",
       "80    34c87972-e471-4e9b-aefc-d5778a9ae505  18/02/1939 00:00   \n",
       "353   34c87972-e471-4e9b-aefc-d5778a9ae505  14/08/1965 00:00   \n",
       "1171  34c87972-e471-4e9b-aefc-d5778a9ae505  14/08/1995 00:00   \n",
       "1307  34c87972-e471-4e9b-aefc-d5778a9ae505  18/02/1939 00:00   \n",
       "1308  34c87972-e471-4e9b-aefc-d5778a9ae505  18/02/1938 00:00   \n",
       "1492  34c87972-e471-4e9b-aefc-d5778a9ae505  22/08/1963 00:00   \n",
       "1609  34c87972-e471-4e9b-aefc-d5778a9ae505  17/06/1938 00:00   \n",
       "1848  34c87972-e471-4e9b-aefc-d5778a9ae505  09/08/1957 00:00   \n",
       "\n",
       "            relationship  \n",
       "0                PARENTS  \n",
       "80                   MOM  \n",
       "353               SISTER  \n",
       "1171         FRIEND_MALE  \n",
       "1307                 MOM  \n",
       "1308                 MOM  \n",
       "1492              SISTER  \n",
       "1609             PARENTS  \n",
       "1848  BEST_FRIEND_FEMALE  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_dt[rel_dt['user_uuid']==inp_json['user_uuid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
