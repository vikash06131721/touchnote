{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Data Scientist Task\n",
    "\n",
    "Given the following data:\n",
    "Messages (Attached)\n",
    "Relationships (Attached)\n",
    "\n",
    "Create a quick model to predict whether or not a message is written for a Birthday and whom it is\n",
    "being sent to, if it is, class it as a Birthday and pull the relationship data for the user id and display the\n",
    "age of the recipient from the relationship data.\n",
    "Output should be something along the lines of:\n",
    "Birthday - Mum - 65\n",
    "Not birthday - Uncle - Nan\n",
    "Birthday - Mum - Nan\n",
    "ETC\n",
    "**To be aware of:\n",
    "1. Please provide the actual code/notebook you used to create this model - preferably through\n",
    "GitHub or similar.\n",
    "2. Should be able to run through an example in real time during the interview (just one\n",
    "message added in with a user id known to have a relationship)\n",
    "3. Think about latency/speed where possible\n",
    "4. Try not to focus too much on accuracy at this point in order to save time in completing the\n",
    "project, however, we will be looking at coding principles to assess\n",
    "5. Please come prepared to answer technical questions on the model, we will be looking for\n",
    "crystal clear communication.\n",
    "6. Not all data will be present for relationship birthdays and so fill with Nan where not possible\n",
    "to get age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "1. Data Exploration\n",
    "2. Data Cleaning\n",
    "3. Merging of different datasets\n",
    "4. Building models, one for birthday.\n",
    "\n",
    "At the end our input should be of the following json format\n",
    "{\"msg\":string,\"user_id\":string)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "1. Basic stats of data for message and relationships\n",
    "2. Dropping Nulls\n",
    "3. Identifying what features can be developed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vikash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vikash/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, f1_score\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.externals import joblib\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import nlpaug.augmenter.word as naw\n",
    "import random\n",
    "import stop_words\n",
    "from nltk.stem import PorterStemmer \n",
    "stops =stop_words.get_stop_words(language='en')\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "ps = PorterStemmer() \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions\n",
    "\n",
    "Functions classes used in the modelling process have been listed down here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_text(x):\n",
    "    text_list =[]\n",
    "    x_eval = eval(x)\n",
    "    for i in range(len(x_eval)):\n",
    "        text_list.append(x_eval[i]['text'])\n",
    "    try:\n",
    "        return ' '.join(text_list)\n",
    "    except TypeError:\n",
    "        return ' '\n",
    "    \n",
    "    \n",
    "def f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    F1 score  for y_true and y_predicted\n",
    "    y_true: True values of array\n",
    "    y_pred: Predicted values from a model\n",
    "    \"\"\"\n",
    "#     pred1 = [p[0] for p in y_pred]\n",
    "#     pred2 = [p[1] for p in y_pred]\n",
    "    f1_rel = f1_score(y_pred,y_true,average='binary')\n",
    "#     f1_birth = f1_score(pred2,y_true['birthday_trgt'],average='binary')\n",
    "    return f1_rel\n",
    "f1_scorer = make_scorer(f1)\n",
    "\n",
    "#feature cleaner\n",
    "class FeatureCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, clean=True):\n",
    "        self.clean = clean\n",
    "        \n",
    "    def clean_and_normalize_text_data(self,sentence):\n",
    "        sentence = re.sub(r'[^a-zA-Z\\s]', ' ', sentence, re.I|re.A)\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.strip()\n",
    "        tokens = wpt.tokenize(sentence)\n",
    "        stemmed_words = [ps.stem(w) for w in tokens]\n",
    "        #remove stopwords\n",
    "        filtered_tokens = [token for token in stemmed_words if token not in stops]\n",
    "        filtered_len =[token for token in filtered_tokens if len(token)>2]\n",
    "        filtered_len = np.unique(filtered_len)\n",
    "        sentence = ' '.join(filtered_len)\n",
    "        return sentence\n",
    "        \n",
    "\n",
    "    def transform(self, X,y=None):\n",
    "        c =pd.Series([self.clean_and_normalize_text_data(x) for x in X])\n",
    "        return c\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    " #statistical features generated   \n",
    "class FeatureMultiplierCount(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, word_count=True,char_count=True,\n",
    "                word_density=True,total_length=True,\n",
    "                capitals=True,caps_vs_length=True,num_exclamation_marks=True,num_question_marks=True,\n",
    "                num_punctuation=True,num_symbols=True,num_unique_words=True,words_vs_unique=True,\n",
    "                word_unique_percent=True):\n",
    "        self.word_count = word_count\n",
    "        self.total_length = total_length\n",
    "        self.char_count =char_count\n",
    "        self.word_density = word_density\n",
    "        self.capitals = capitals\n",
    "        self.caps_vs_length = caps_vs_length\n",
    "        self.num_exclamation_marks=num_exclamation_marks\n",
    "        self.num_question_marks=num_question_marks\n",
    "        self.num_punctuation=num_punctuation\n",
    "        self.num_symbols=num_symbols\n",
    "        self.num_unique_words = num_unique_words\n",
    "        self.words_vs_unique = words_vs_unique\n",
    "        self.word_unique_percent = word_unique_percent\n",
    "\n",
    "    def transform(self, X,y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        X['word_count'] = X['all_text_new'].apply(lambda x : len(x.split()))\n",
    "        X['char_count'] = X['all_text_new'].apply(lambda x : len(x.replace(\" \",\"\")))\n",
    "        X['word_density'] = X['word_count'] / (X['char_count'] + 1)\n",
    "\n",
    "        X['total_length'] = X['all_text_new'].apply(len)\n",
    "        X['capitals'] = X['all_text_new'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    " \n",
    "        X['num_exclamation_marks'] =X['all_text_new'].apply(lambda x: x.count('!'))\n",
    "        X['num_question_marks'] = X['all_text_new'].apply(lambda x: x.count('?'))\n",
    "        X['num_punctuation'] = X['all_text_new'].apply(lambda x: sum(x.count(w) for w in '.,;:'))\n",
    "        X['num_symbols'] = X['all_text_new'].apply(lambda x: sum(x.count(w) for w in '*&$%'))\n",
    "        X['num_unique_words'] = X['all_text_new'].apply(lambda x: len(set(w for w in x.split())))\n",
    "       \n",
    "        \n",
    "        return X[['word_count','char_count','word_density','total_length',\n",
    "                 'capitals','num_exclamation_marks','num_question_marks',\n",
    "                 'num_punctuation','num_symbols','num_unique_words']]\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "\n",
    "#modelling pipeline \n",
    "def models():\n",
    "    \"\"\"\n",
    "    \n",
    "    returns three pipelines Random Forest, Adaboost, Gradientboost,SVC\n",
    "    The scoring function is based on f1_score\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Random Forest Pipeline\n",
    "    rf_pipeline = Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([('clean',FeatureCleaner()),\n",
    "             ('tfidf', TfidfVectorizer(max_features=10000,ngram_range=(1,3))),\n",
    "        ])),\n",
    "        ('numerical_features',Pipeline([('numerical_feats',FeatureMultiplierCount()),\n",
    "                                       ('scaler',StandardScaler()),\n",
    "                                       ])),\n",
    "\n",
    "    ])),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "\n",
    "])\n",
    "    # Adaboost Pipeline\n",
    "    AdaBoost_pipeline = Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([('clean',FeatureCleaner()),\n",
    "             ('tfidf', TfidfVectorizer(max_features=10000,ngram_range=(1,3))),\n",
    "        ])),\n",
    "        ('numerical_features',Pipeline([('numerical_feats',FeatureMultiplierCount()),\n",
    "                                       ('scaler',StandardScaler()),\n",
    "                                       ])),\n",
    "\n",
    "    ])),\n",
    "    ('clf', AdaBoostClassifier()),\n",
    "\n",
    "])\n",
    "    # Gradient Boost Pipeline\n",
    "    GRD_pipeline = Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([('clean',FeatureCleaner()),\n",
    "             ('tfidf', TfidfVectorizer(max_features=10000,ngram_range=(1,3))),\n",
    "        ])),\n",
    "        ('numerical_features',Pipeline([('numerical_feats',FeatureMultiplierCount()),\n",
    "                                       ('scaler',StandardScaler()),\n",
    "                                       ])),\n",
    "\n",
    "    ])),\n",
    "    ('clf', GradientBoostingClassifier()),\n",
    "\n",
    "])\n",
    "    \n",
    "    svm_pipeline =Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([('clean',FeatureCleaner()),\n",
    "             ('tfidf', TfidfVectorizer(max_features=10000,ngram_range=(1,3))),\n",
    "        ])),\n",
    "        ('numerical_features',Pipeline([('numerical_feats',FeatureMultiplierCount()),\n",
    "                                       ('scaler',StandardScaler()),\n",
    "                                       ])),\n",
    "\n",
    "    ])),\n",
    "    ('clf', LinearSVC()),\n",
    "\n",
    "])\n",
    "    #grid search params for randomforest, adaboost, gradientboost\n",
    "    grid_params_rf = [{'clf__n_estimators': [10, 50, 100], 'clf__max_depth': [2, 3, 5]}]\n",
    "    grid_params_adaboost = [{'clf__n_estimators': [10, 50, 100,500], 'clf__learning_rate': [0.5, 0.8, 1.0]}]\n",
    "    grid_params_grd = [{'clf__n_estimators': [10, 50, 100,500], 'clf__learning_rate': [0.5, 0.8, 1.0],\n",
    "                            'clf__max_depth': [2, 3, 5]}]\n",
    "    \n",
    "    grid_params_svc = [{'clf__C': [1.0,3.0,5.0,10.0],'clf__max_iter':[1000]}]\n",
    "\n",
    "    #gridsearchcv pipeline for randomforest\n",
    "    gs_rf = GridSearchCV(estimator=rf_pipeline,\n",
    "                             param_grid=grid_params_rf,\n",
    "                             scoring=f1_scorer,\n",
    "                             cv=5)\n",
    "    #gridsearchcv pipeline for adaboost\n",
    "    gs_adaboost = GridSearchCV(estimator=AdaBoost_pipeline,\n",
    "                                   param_grid=grid_params_adaboost,\n",
    "                                   scoring=f1_scorer,\n",
    "                                   cv=5)\n",
    "\n",
    "    #gridsearchcv pipeline for gradientboost\n",
    "    gs_grd = GridSearchCV(estimator=GRD_pipeline,\n",
    "                              param_grid=grid_params_grd,\n",
    "                              scoring=f1_scorer,\n",
    "                              cv=5)\n",
    "    gs_svc = GridSearchCV(estimator=svm_pipeline,\n",
    "                              param_grid=grid_params_svc,\n",
    "                              scoring=f1_scorer,\n",
    "                              cv=5)\n",
    "\n",
    "    grids = [gs_svc,gs_rf, gs_adaboost, gs_grd]\n",
    "    # grids = [gs_rf]\n",
    "    return grids\n",
    "\n",
    "\n",
    "def clean_and_normalize_text_data(sent):\n",
    "\n",
    "\n",
    "    \n",
    "    sent_cleaned = sent.replace('\\n','')\n",
    "    return sent_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_uuid</th>\n",
       "      <th>message</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>[{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>[{\"text\":\"It's going well....\",\"fontIntegratio...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f841ccb1-3599-a847-d99b-150a9cb19858</td>\n",
       "      <td>[{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>[{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4407500-2e8e-8059-04b5-12adb394ac0d</td>\n",
       "      <td>[{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              user_uuid  \\\n",
       "0  34c87972-e471-4e9b-aefc-d5778a9ae505   \n",
       "1  117fd8a2-5da2-8b13-6df0-d6766d849093   \n",
       "2  f841ccb1-3599-a847-d99b-150a9cb19858   \n",
       "3  117fd8a2-5da2-8b13-6df0-d6766d849093   \n",
       "4  d4407500-2e8e-8059-04b5-12adb394ac0d   \n",
       "\n",
       "                                             message Unnamed: 2  \n",
       "0  [{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...        NaN  \n",
       "1  [{\"text\":\"It's going well....\",\"fontIntegratio...        NaN  \n",
       "2  [{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...        NaN  \n",
       "3  [{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...        NaN  \n",
       "4  [{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...        NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_dt = pd.read_csv('Message.csv')\n",
    "rel = pd.read_csv('relationships.csv')\n",
    "msg_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      "user_uuid     10000 non-null object\n",
      "message       10000 non-null object\n",
      "Unnamed: 2    346 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#basic info about message data\n",
    "msg_dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQUE USER IDS IN MESSAGE DATA 8600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cb66eaa6-cb85-f180-f2df-b26b91072e76    83\n",
       "45903ea1-3d81-7f7d-25cd-9acc5c4570f3    35\n",
       "65dc5608-0fde-241f-e963-9ed255703a83    21\n",
       "4b2b3920-0498-100e-6022-5d29ac36bdb2    17\n",
       "3f037c21-dbe1-cb56-c080-1e28d71f0543    16\n",
       "                                        ..\n",
       "ba0e326c-e465-419e-b6d1-056683ea2347     1\n",
       "ebba6f18-3ff4-4748-8f09-4ae7973037d6     1\n",
       "df841ee5-72ef-8c41-8623-ef6dfc218dcd     1\n",
       "905bd8ff-56dc-4cb0-b766-61d300544afa     1\n",
       "b2eb420b-4794-4669-80f3-378d00b9682c     1\n",
       "Name: user_uuid, Length: 8600, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"UNIQUE USER IDS IN MESSAGE DATA {}\".format(msg_dt['user_uuid'].nunique()))\n",
    "msg_dt['user_uuid'].value_counts()\n",
    "#the value counts on user ids shows the top users with their number of messages sent.\n",
    "#lets explore on few users using the relationship data where they most likely use touchnote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_uuid</th>\n",
       "      <th>message</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>message_new</th>\n",
       "      <th>all_text</th>\n",
       "      <th>all_text_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>[{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"Dear\",\"fontIntegration\":0,\"font\":\"Bl...</td>\n",
       "      <td>Dear Granddad Get well soon!\\n\\nWe love you th...</td>\n",
       "      <td>Dear Granddad Get well soon!We love you the wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>[{\"text\":\"It's going well....\",\"fontIntegratio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"It's going well....\",\"fontIntegratio...</td>\n",
       "      <td>It's going well....</td>\n",
       "      <td>It's going well....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f841ccb1-3599-a847-d99b-150a9cb19858</td>\n",
       "      <td>[{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...</td>\n",
       "      <td>Liebe Rita &amp; Claudia,\\n\\nliebe und sonnige Grü...</td>\n",
       "      <td>Liebe Rita &amp; Claudia,liebe und sonnige Grüße a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              user_uuid  \\\n",
       "0  34c87972-e471-4e9b-aefc-d5778a9ae505   \n",
       "1  117fd8a2-5da2-8b13-6df0-d6766d849093   \n",
       "2  f841ccb1-3599-a847-d99b-150a9cb19858   \n",
       "\n",
       "                                             message Unnamed: 2  \\\n",
       "0  [{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...        NaN   \n",
       "1  [{\"text\":\"It's going well....\",\"fontIntegratio...        NaN   \n",
       "2  [{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...        NaN   \n",
       "\n",
       "                                         message_new  \\\n",
       "0  [{\"text\":\"Dear\",\"fontIntegration\":0,\"font\":\"Bl...   \n",
       "1  [{\"text\":\"It's going well....\",\"fontIntegratio...   \n",
       "2  [{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...   \n",
       "\n",
       "                                            all_text  \\\n",
       "0  Dear Granddad Get well soon!\\n\\nWe love you th...   \n",
       "1                                It's going well....   \n",
       "2  Liebe Rita & Claudia,\\n\\nliebe und sonnige Grü...   \n",
       "\n",
       "                                        all_text_new  \n",
       "0  Dear Granddad Get well soon!We love you the wo...  \n",
       "1                                It's going well....  \n",
       "2  Liebe Rita & Claudia,liebe und sonnige Grüße a...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we format the message data to extract all the text to be used further in modelling, removing the notorious '\\n' also\n",
    "msg_dt['message_new'] = msg_dt['message'].apply(lambda x: x.replace('null',str(0)))\n",
    "msg_dt['all_text_new'] = msg_dt['message_new'].apply(lambda x:extract_all_text(x))\n",
    "msg_dt['all_text_new'] = msg_dt['all_text_new'].apply(lambda x:clean_and_normalize_text_data(x))\n",
    "msg_dt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_uuid</th>\n",
       "      <th>birthday</th>\n",
       "      <th>relationship</th>\n",
       "      <th>relationship_new</th>\n",
       "      <th>birthday_trgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>17/06/1938 00:00</td>\n",
       "      <td>PARENTS</td>\n",
       "      <td>PARENTS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MOM</td>\n",
       "      <td>MOM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f841ccb1-3599-a847-d99b-150a9cb19858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SISTER_IN_LAW</td>\n",
       "      <td>SISTER_IN_LAW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MOM</td>\n",
       "      <td>MOM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4407500-2e8e-8059-04b5-12adb394ac0d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DAD</td>\n",
       "      <td>DAD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              user_uuid          birthday   relationship  \\\n",
       "0  34c87972-e471-4e9b-aefc-d5778a9ae505  17/06/1938 00:00        PARENTS   \n",
       "1  117fd8a2-5da2-8b13-6df0-d6766d849093               NaN            MOM   \n",
       "2  f841ccb1-3599-a847-d99b-150a9cb19858               NaN  SISTER_IN_LAW   \n",
       "3  117fd8a2-5da2-8b13-6df0-d6766d849093               NaN            MOM   \n",
       "4  d4407500-2e8e-8059-04b5-12adb394ac0d               NaN            DAD   \n",
       "\n",
       "  relationship_new  birthday_trgt  \n",
       "0          PARENTS              1  \n",
       "1              MOM              0  \n",
       "2    SISTER_IN_LAW              0  \n",
       "3              MOM              0  \n",
       "4              DAD              0  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#relationship data\n",
    "rel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_uuid                             relationship      \n",
       "45903ea1-3d81-7f7d-25cd-9acc5c4570f3  BEST_FRIEND_FEMALE     1\n",
       "                                      Blipfoto Friend        6\n",
       "                                      COLLEAGUE_FEMALE       1\n",
       "                                      COLLEAGUE_MALE         1\n",
       "                                      COUSIN                 3\n",
       "                                      Church Friend          3\n",
       "                                      GRANDSON               8\n",
       "                                      MS Friend              2\n",
       "                                      Neighbour              1\n",
       "                                      SISTER                 1\n",
       "                                      Sister-in-law          1\n",
       "                                      Visiting Sister        7\n",
       "65dc5608-0fde-241f-e963-9ed255703a83  BEST_FRIEND_FEMALE    19\n",
       "                                      FRIEND_FEMALE          2\n",
       "cb66eaa6-cb85-f180-f2df-b26b91072e76  BEST_FRIEND_FEMALE     2\n",
       "                                      BEST_FRIEND_MALE       3\n",
       "                                      BOYFRIEND              3\n",
       "                                      COLLEAGUE_FEMALE       2\n",
       "                                      COLLEAGUE_MALE         6\n",
       "                                      DAD                   19\n",
       "                                      FRIENDS                7\n",
       "                                      FRIEND_FEMALE          2\n",
       "                                      FRIEND_MALE            2\n",
       "                                      GIRLFRIEND             1\n",
       "                                      GRANDMA                9\n",
       "                                      MOM                   16\n",
       "                                      NURSE                  8\n",
       "                                      PARENTS                2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting couple of top users \n",
    "users  =['cb66eaa6-cb85-f180-f2df-b26b91072e76','45903ea1-3d81-7f7d-25cd-9acc5c4570f3','65dc5608-0fde-241f-e963-9ed255703a83']\n",
    "user_relationship = rel[rel['user_uuid'].isin(users)]\n",
    "user_relationship.groupby(['user_uuid','relationship']).size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights About The Users, Can it be automated ?\n",
    "\n",
    "1. 45903ea1-3d81-7f7d-25cd-9acc5c4570f3, the relationship reveals a lot about the age intervals, the user has messaged 8times to 'GRANDSON' the age can be put in the interval of (>45). Though anomalies can always happen. The social structure is also kind of predictive, from the messaging list we can observe anomaly relationship like \"Church Friend\", in this world of very private relationships, having a \"Church Friend\" reveals a very wide and friendly network.\n",
    "\n",
    "2. 65dc5608-0fde-241f-e963-9ed255703a83, in contrast to the above user this particular user has a very secluded list, not much to get from here.\n",
    "\n",
    "3. cb66eaa6-cb85-f180-f2df-b26b91072e76, this user is probably young \"GRANDMA\" and \"GIRLFRIEND\" indicates that and is not married. He was also probably in hospital recently(\"NURSE\").\n",
    "\n",
    "These insights can be automated in multiple ways:\n",
    "1. Connecting nodes(relationships) to messages for each user with weights as number of times the message has been sent, in turn entities from these messages are connected to all users.\n",
    "\n",
    "This type of graphical relationship will allow us to harness these insights in automated way.\n",
    "\n",
    "Here I would also like to comment that having message_id to each message and connecting the message_id to relationship\n",
    "would have greater influence. Since a user might message multiple times, and which message corresponds to which relationship will have richer represenatation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9892, 6)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtering messages with no text\n",
    "msg_dt = msg_dt[msg_dt['all_text_new']!=' ']\n",
    "msg_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_uuid</th>\n",
       "      <th>message</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>message_new</th>\n",
       "      <th>all_text</th>\n",
       "      <th>all_text_new</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>[{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"Dear\",\"fontIntegration\":0,\"font\":\"Bl...</td>\n",
       "      <td>Dear Granddad Get well soon!\\n\\nWe love you th...</td>\n",
       "      <td>Dear Granddad Get well soon!We love you the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>[{\"text\":\"It's going well....\",\"fontIntegratio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"It's going well....\",\"fontIntegratio...</td>\n",
       "      <td>It's going well....</td>\n",
       "      <td>It's going well....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f841ccb1-3599-a847-d99b-150a9cb19858</td>\n",
       "      <td>[{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...</td>\n",
       "      <td>Liebe Rita &amp; Claudia,\\n\\nliebe und sonnige Grü...</td>\n",
       "      <td>Liebe Rita &amp; Claudia,liebe und sonnige Grüße a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>[{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...</td>\n",
       "      <td>\\n\\n\\n\\nYou can't see the chilli and parsnip c...</td>\n",
       "      <td>You can't see the chilli and parsnip chutney a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4407500-2e8e-8059-04b5-12adb394ac0d</td>\n",
       "      <td>[{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...</td>\n",
       "      <td>¡Hola papá!\\n\\nMira qué vistas desde lo alto d...</td>\n",
       "      <td>¡Hola papá!Mira qué vistas desde lo alto del P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              user_uuid  \\\n",
       "0  34c87972-e471-4e9b-aefc-d5778a9ae505   \n",
       "1  117fd8a2-5da2-8b13-6df0-d6766d849093   \n",
       "2  f841ccb1-3599-a847-d99b-150a9cb19858   \n",
       "3  117fd8a2-5da2-8b13-6df0-d6766d849093   \n",
       "4  d4407500-2e8e-8059-04b5-12adb394ac0d   \n",
       "\n",
       "                                             message Unnamed: 2  \\\n",
       "0  [{\"text\":\"Dear\",\"fontIntegration\":null,\"font\":...        NaN   \n",
       "1  [{\"text\":\"It's going well....\",\"fontIntegratio...        NaN   \n",
       "2  [{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...        NaN   \n",
       "3  [{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...        NaN   \n",
       "4  [{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...        NaN   \n",
       "\n",
       "                                         message_new  \\\n",
       "0  [{\"text\":\"Dear\",\"fontIntegration\":0,\"font\":\"Bl...   \n",
       "1  [{\"text\":\"It's going well....\",\"fontIntegratio...   \n",
       "2  [{\"text\":\"Liebe Rita \\u0026 Claudia,\\n\\nliebe ...   \n",
       "3  [{\"text\":\"\\n\\n\\n\\nYou can't see the chilli and...   \n",
       "4  [{\"text\":\"¡Hola papá!\\n\\nMira qué vistas desde...   \n",
       "\n",
       "                                            all_text  \\\n",
       "0  Dear Granddad Get well soon!\\n\\nWe love you th...   \n",
       "1                                It's going well....   \n",
       "2  Liebe Rita & Claudia,\\n\\nliebe und sonnige Grü...   \n",
       "3  \\n\\n\\n\\nYou can't see the chilli and parsnip c...   \n",
       "4  ¡Hola papá!\\n\\nMira qué vistas desde lo alto d...   \n",
       "\n",
       "                                        all_text_new  target  \n",
       "0  Dear Granddad Get well soon!We love you the wo...       0  \n",
       "1                                It's going well....       0  \n",
       "2  Liebe Rita & Claudia,liebe und sonnige Grüße a...       0  \n",
       "3  You can't see the chilli and parsnip chutney a...       0  \n",
       "4  ¡Hola papá!Mira qué vistas desde lo alto del P...       0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a birthday target variable\n",
    "def brth_(x):\n",
    "    split_ = x.split(' ')\n",
    "    if len(set(split_).intersection(set(['birthday','Birthday'])))>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "msg_dt['target'] = msg_dt['all_text_new'].apply(lambda x:brth_(x))\n",
    "msg_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.91943\n",
      "1    0.08057\n",
      "Name: target, dtype: float64\n",
      "(9892, 7)\n"
     ]
    }
   ],
   "source": [
    "#target variables percentage\n",
    "print (msg_dt['target'].value_counts(normalize=True))\n",
    "print (msg_dt.shape)\n",
    "#they seem okay to proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_dt = msg_dt[['user_uuid','target','all_text_new']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Augmentation For Oversampling\n",
    "\n",
    "In traditional modelling methods we used various oversampling methods after featurization to increase the undersampled classes.\n",
    "But in modern methods the concept of text augmentation has come around to good usage. The concept of text augmentation is to generate similar meaning text but by replacing new words so that we can have more text data for undersampled classes. We can use the following approaches for text augmentation to generate good relevant text:\n",
    "\n",
    "1. Replace some words by their synonyms\n",
    "2. Replace some words by similar words(like love is replaced beloved)\n",
    "3. Translate the sentence to other language and then translate it back to english\n",
    "\n",
    "We will be using the second number point. We will pull every undersampled data to atleast 10%(around 200) of the highest class is they are'nt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we try to keep the classes which have lower data to atleast 30% i.e 2966 datapoints of the highest class, i.e each class must have 20% of highest class of data.\n",
    "#for that we augment text using synonyms like below\n",
    "aug  = naw.SynonymAug(aug_src='wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augmented_text(classes):\n",
    "    df = msg_dt[msg_dt['target']==classes]\n",
    "    print (df.shape)\n",
    "    all_ids = df['user_uuid'].to_list()\n",
    "    max_num =2966\n",
    "    to_augment = max_num-df.shape[0]\n",
    "    print (to_augment)\n",
    "    new_df = pd.DataFrame()\n",
    "    id_chosen =[]\n",
    "    i=0\n",
    "    while i<to_augment:\n",
    "        id_ = random.choice(list(set(all_ids)-set(id_chosen)))\n",
    "        sent = df[df['user_uuid']==id_]['all_text_new'].to_list()[0]\n",
    "        sent_generated = aug.augment(sent)\n",
    "        temp = pd.DataFrame(columns=['user_uuid','target','all_text_new'],index=[0])\n",
    "        temp['user_uuid'] =id_\n",
    "        temp['target'] = classes\n",
    "      \n",
    "        temp['all_text_new'] = sent_generated\n",
    "        new_df = new_df.append(temp)\n",
    "        i=i+1\n",
    "    return pd.concat([df,new_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(797, 3)\n",
      "2169\n"
     ]
    }
   ],
   "source": [
    "aug_class =[1]\n",
    "\n",
    "new_aug_dt =[]\n",
    "for cl in aug_class:\n",
    "    df = generate_augmented_text(cl)\n",
    "    new_aug_dt.append(df)\n",
    "train_dt = msg_dt[~msg_dt['target'].isin(aug_class)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12061, 3)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after oversampling with text augmentation\n",
    "all_dt = pd.concat([train_dt,new_aug_dt[0]])\n",
    "all_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.754083\n",
       "1    0.245917\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dt['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is the features on which we develop and y is the target variable\n",
    "y = all_dt.target\n",
    "X = all_dt.all_text_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test and train in 75% and 25% ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 9095, 1: 2966})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization And Modelling\n",
    "\n",
    "1. For featurization we are using the following features:\n",
    " 1. TFIDF(10000) features with ngram(1,3) \n",
    " 2. Counting features\n",
    " 3. Scaling the count features \n",
    " 4. Combining all the features\n",
    "\n",
    "2. Modelling\n",
    "   1. We use LinearSVC, Randomforest, Gradientboost, Adaboost to train the model\n",
    "   2. We run our gridsearchcv with cv=5 \n",
    "   3. We choose the best model on F1 score\n",
    "   \n",
    "All this is being done in a single pipeline(refer models function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimator: svc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__C': 1.0, 'clf__max_iter': 1000}\n",
      "Best training F1 score  score: 0.934\n",
      "Test set f1score for best params: 0.933 \n",
      "\n",
      "Estimator: rf\n",
      "Best params: {'clf__max_depth': 5, 'clf__n_estimators': 10}\n",
      "Best training F1 score  score: 0.030\n",
      "Test set f1score for best params: 0.000 \n",
      "\n",
      "Estimator: ada\n",
      "Best params: {'clf__learning_rate': 0.5, 'clf__n_estimators': 50}\n",
      "Best training F1 score  score: 0.927\n",
      "Test set f1score for best params: 0.912 \n",
      "\n",
      "Estimator: grd\n",
      "Best params: {'clf__learning_rate': 0.5, 'clf__max_depth': 3, 'clf__n_estimators': 10}\n",
      "Best training F1 score  score: 0.924\n",
      "Test set f1score for best params: 0.906 \n",
      "\\Classifier with best test set f1 score: svc\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "grid_dict = {0:'svc',1:'rf',2:'ada',3:'grd'}\n",
    "\n",
    "    # grid_dict = {0: 'rf'}\n",
    "\n",
    "grids =models()\n",
    "\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    print('Best training F1 score  score: %.3f' % gs.best_score_)\n",
    "    y_pred = gs.predict(X_test)\n",
    "    print('Test set f1score for best params: %.3f ' % f1(y_test, y_pred))\n",
    "    if f1(y_test, y_pred) >best_acc:\n",
    "        best_acc = f1(y_test, y_pred)\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\Classifier with best test set f1 score: %s' % grid_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "# since the best pipeline is with LinearSVC we will run and store it on the complete dataset\n",
    "svm_pipeline =Pipeline([\n",
    "    ('u1', FeatureUnion([\n",
    "        ('tfdif_features', Pipeline([('clean',FeatureCleaner()),\n",
    "             ('tfidf', TfidfVectorizer(max_features=10000,ngram_range=(1,3))),\n",
    "        ])),\n",
    "        ('numerical_features',Pipeline([('numerical_feats',FeatureMultiplierCount()),\n",
    "                                       ('scaler',StandardScaler()),\n",
    "                                       ])),\n",
    "\n",
    "    ])),\n",
    "    ('clf', LinearSVC()),\n",
    "\n",
    "])\n",
    "grid_params_svc = [{'clf__C': [1.0,3.0,5.0,10.0],'clf__max_iter':[1000]}]\n",
    "#gridsearchcv pipeline for LinearSVC\n",
    "gs_svc = GridSearchCV(estimator=svm_pipeline,\n",
    "                              param_grid=grid_params_svc,\n",
    "                              scoring=f1_scorer,\n",
    "                              cv=5)\n",
    "\n",
    "model = gs_svc.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9259141836256134"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_gs_pipeline_brthday_svc.pkl']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_file = 'best_gs_pipeline_brthday_svc.pkl'\n",
    "\n",
    "\n",
    "joblib.dump(model.best_estimator_,  dump_file, compress=1)\n",
    "\n",
    "# with open(model.best_estimator_, 'wb') as file:\n",
    "#     pickle.dump(best_gs, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib_clf = joblib.load(dump_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HAPPY BIRTHDAY! I love you cousin Ella! ! Seminal fluid gambling in my ball quarry with me, we cause sooooo much fun! Anyway has a happy birthday I hope you get lots of cake! ! Love, Aria'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dt = pd.read_csv(\"relationships.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_uuid</th>\n",
       "      <th>birthday</th>\n",
       "      <th>relationship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>17/06/1938 00:00</td>\n",
       "      <td>PARENTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>18/02/1939 00:00</td>\n",
       "      <td>MOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>14/08/1965 00:00</td>\n",
       "      <td>SISTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>14/08/1995 00:00</td>\n",
       "      <td>FRIEND_MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>18/02/1939 00:00</td>\n",
       "      <td>MOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>18/02/1938 00:00</td>\n",
       "      <td>MOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>22/08/1963 00:00</td>\n",
       "      <td>SISTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>17/06/1938 00:00</td>\n",
       "      <td>PARENTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>09/08/1957 00:00</td>\n",
       "      <td>BEST_FRIEND_FEMALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 user_uuid          birthday  \\\n",
       "0     34c87972-e471-4e9b-aefc-d5778a9ae505  17/06/1938 00:00   \n",
       "80    34c87972-e471-4e9b-aefc-d5778a9ae505  18/02/1939 00:00   \n",
       "353   34c87972-e471-4e9b-aefc-d5778a9ae505  14/08/1965 00:00   \n",
       "1171  34c87972-e471-4e9b-aefc-d5778a9ae505  14/08/1995 00:00   \n",
       "1307  34c87972-e471-4e9b-aefc-d5778a9ae505  18/02/1939 00:00   \n",
       "1308  34c87972-e471-4e9b-aefc-d5778a9ae505  18/02/1938 00:00   \n",
       "1492  34c87972-e471-4e9b-aefc-d5778a9ae505  22/08/1963 00:00   \n",
       "1609  34c87972-e471-4e9b-aefc-d5778a9ae505  17/06/1938 00:00   \n",
       "1848  34c87972-e471-4e9b-aefc-d5778a9ae505  09/08/1957 00:00   \n",
       "\n",
       "            relationship  \n",
       "0                PARENTS  \n",
       "80                   MOM  \n",
       "353               SISTER  \n",
       "1171         FRIEND_MALE  \n",
       "1307                 MOM  \n",
       "1308                 MOM  \n",
       "1492              SISTER  \n",
       "1609             PARENTS  \n",
       "1848  BEST_FRIEND_FEMALE  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_dt[rel_dt['user_uuid']==inp_json['user_uuid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT To Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_uuid</th>\n",
       "      <th>target</th>\n",
       "      <th>all_text_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34c87972-e471-4e9b-aefc-d5778a9ae505</td>\n",
       "      <td>0</td>\n",
       "      <td>Dear Granddad Get well soon!We love you the wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>0</td>\n",
       "      <td>It's going well....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f841ccb1-3599-a847-d99b-150a9cb19858</td>\n",
       "      <td>0</td>\n",
       "      <td>Liebe Rita &amp; Claudia,liebe und sonnige Grüße a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117fd8a2-5da2-8b13-6df0-d6766d849093</td>\n",
       "      <td>0</td>\n",
       "      <td>You can't see the chilli and parsnip chutney a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4407500-2e8e-8059-04b5-12adb394ac0d</td>\n",
       "      <td>0</td>\n",
       "      <td>¡Hola papá!Mira qué vistas desde lo alto del P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              user_uuid  target  \\\n",
       "0  34c87972-e471-4e9b-aefc-d5778a9ae505       0   \n",
       "1  117fd8a2-5da2-8b13-6df0-d6766d849093       0   \n",
       "2  f841ccb1-3599-a847-d99b-150a9cb19858       0   \n",
       "3  117fd8a2-5da2-8b13-6df0-d6766d849093       0   \n",
       "4  d4407500-2e8e-8059-04b5-12adb394ac0d       0   \n",
       "\n",
       "                                        all_text_new  \n",
       "0  Dear Granddad Get well soon!We love you the wo...  \n",
       "1                                It's going well....  \n",
       "2  Liebe Rita & Claudia,liebe und sonnige Grüße a...  \n",
       "3  You can't see the chilli and parsnip chutney a...  \n",
       "4  ¡Hola papá!Mira qué vistas desde lo alto del P...  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, temp_text, train_labels, temp_labels = train_test_split(all_dt['all_text_new'], all_dt['target'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=all_dt['target'])\n",
    "\n",
    "# we will use temp_text and temp_labels to create validation and test set\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aba1c71d9db4eceaa4975b0f7e15d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=433.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590fea9444dc4a109ee0436e23e22d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf8f0cb893f4137a07120c07e27c9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466062.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
    "\n",
    "# encode text\n",
    "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\n",
    "# output\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#if we keep 125 we will cover most of the text\n",
    "# tokenize and encode sequences in the training set\n",
    "max_seq_len=125\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "        \n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66305372 2.03323699]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikash/miniconda3/envs/pytorch_coding/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass classes=[0 1], y=4585    0\n",
      "4160    0\n",
      "6780    0\n",
      "7940    0\n",
      "9920    0\n",
      "       ..\n",
      "1192    0\n",
      "0       1\n",
      "2641    0\n",
      "6653    0\n",
      "4980    0\n",
      "Name: target, Length: 8442, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "\n",
    "print(class_wts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between a\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    " # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "#       elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "      \n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     57.\n",
      "\n",
      "Training Loss: 0.610\n",
      "Validation Loss: 0.552\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     57.\n",
      "\n",
      "Training Loss: 0.588\n",
      "Validation Loss: 0.542\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     57.\n",
      "\n",
      "Training Loss: 0.572\n",
      "Validation Loss: 0.525\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     57.\n",
      "\n",
      "Training Loss: 0.559\n",
      "Validation Loss: 0.534\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     57.\n",
      "\n",
      "Training Loss: 0.545\n",
      "Validation Loss: 0.517\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     57.\n",
      "\n",
      "Training Loss: 0.555\n",
      "Validation Loss: 0.502\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     57.\n",
      "\n",
      "Training Loss: 0.538\n",
      "Validation Loss: 0.601\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     57.\n",
      "\n",
      "Training Loss: 0.548\n",
      "Validation Loss: 0.514\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     57.\n",
      "\n",
      "Training Loss: 0.523\n",
      "Validation Loss: 0.489\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     57.\n",
      "\n",
      "Training Loss: 0.524\n",
      "Validation Loss: 0.492\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_Arch(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to('cpu'), test_mask.to('cpu'))\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.73      0.81      1365\n",
      "           1       0.48      0.77      0.59       445\n",
      "\n",
      "    accuracy                           0.74      1810\n",
      "   macro avg       0.69      0.75      0.70      1810\n",
      "weighted avg       0.80      0.74      0.76      1810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model's performance\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ultimately its proven that LinearSVC performs well even than BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
